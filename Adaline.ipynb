{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXnntaRK79dO3I7XSlwBYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeTZ25/felipe/blob/main/Adaline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPUwZCXbkWh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0c02a176-34bb-44a4-f491-3f1ee847144c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-20372611f31a>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#fig, a x = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mada1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdalineGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mada1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mada1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# ## Implementar Adaline en Python\n",
        "# In[17]:\n",
        "\n",
        "class AdalineGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "        self.cost_ = []\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            net_input = self.net_input(X)\n",
        "            # Ten en cuenta que el método \"activación\" no produce ningún efecto\n",
        "            # sobre el código, puesto que es simplemente una función de identidad.\n",
        "            # En su lugar, podemos escribir directamente `output = self.net_input(X)`.\n",
        "            # El objetivo de la activación es más conceptual, por ejemplo,\n",
        "            # en el caso de una regresión logística (como veremos más tarde),\n",
        "            # podríamos cambiarla a\n",
        "            # una función sigmoide para implementar un clasificador de regresión logística.\n",
        "            output = self.activation(net_input)\n",
        "            errors = (y - output)\n",
        "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
        "            self.w_[0] += self.eta * errors.sum()\n",
        "            cost = (errors**2).sum() / 2.0\n",
        "            self.cost_.append(cost)\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n",
        "\n",
        "# In[18]:\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "\n",
        "ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)\n",
        "ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('log(Sum-squared-error)')\n",
        "ax[0].set_title('Adaline - Learning rate 0.01')\n",
        "\n",
        "ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)\n",
        "ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Sum-squared-error')\n",
        "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
        "\n",
        "# plt.savefig('images/02_11.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# In[19]:\n",
        "# ## Mejorar el descenso de gradiente mediante el escalado de características\n",
        "# In[20]:\n",
        "# In[21]:\n",
        "# estandarizar características\n",
        "X_std = np.copy(X)\n",
        "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
        "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "ada = AdalineGD(n_iter=15, eta=0.01)\n",
        "ada.fit(X_std, y)\n",
        "\n",
        "plot_decision_regions(X_std, y, classifier=ada)\n",
        "plt.title('Adaline - Gradient Descent')\n",
        "plt.xlabel('sepal length [standardized]')\n",
        "plt.ylabel('petal length [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_14_1.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Sum-squared-error')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_14_2.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ## Aprendizaje automático a gran escala y descenso de gradiente estocástico\n",
        "# In[23]:\n",
        "\n",
        "class AdalineSGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    shuffle : bool (default: True)\n",
        "      Shuffles training data every epoch if True to prevent cycles.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value averaged over all\n",
        "      training samples in each epoch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.w_initialized = False\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.cost_ = []\n",
        "        for i in range(self.n_iter):\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "            cost = []\n",
        "            for xi, target in zip(X, y):\n",
        "                cost.append(self._update_weights(xi, target))\n",
        "            avg_cost = sum(cost) / len(y)\n",
        "            self.cost_.append(avg_cost)\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y):\n",
        "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
        "        if not self.w_initialized:\n",
        "            self._initialize_weights(X.shape[1])\n",
        "        if y.ravel().shape[0] > 1:\n",
        "            for xi, target in zip(X, y):\n",
        "                self._update_weights(xi, target)\n",
        "        else:\n",
        "            self._update_weights(X, y)\n",
        "        return self\n",
        "\n",
        "    def _shuffle(self, X, y):\n",
        "        \"\"\"Shuffle training data\"\"\"\n",
        "        r = self.rgen.permutation(len(y))\n",
        "        return X[r], y[r]\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
        "        self.w_initialized = True\n",
        "\n",
        "    def _update_weights(self, xi, target):\n",
        "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
        "        output = self.activation(self.net_input(xi))\n",
        "        error = (target - output)\n",
        "        self.w_[1:] += self.eta * xi.dot(error)\n",
        "        self.w_[0] += self.eta * error\n",
        "        cost = 0.5 * error**2\n",
        "        return cost\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\n",
        "ada.fit(X_std, y)\n",
        "\n",
        "plot_decision_regions(X_std, y, classifier=ada)\n",
        "plt.title('Adaline - Stochastic Gradient Descent')\n",
        "plt.xlabel('sepal length [standardized]')\n",
        "plt.ylabel('petal length [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_1.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Cost')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_2.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "ada.partial_fit(X_std[0, :], y[0])"
      ]
    }
  ]
}